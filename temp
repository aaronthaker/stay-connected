I work for PwC in Financial Crime and I am working on a project for Glencore. They have hired PwC to implement trade and communications surveillance systems so that they can be alerted when traders have potentially committed insider trading, market abuse etc. I am currently working on the communications surveillance part of this project. On the communications surveillance part, we are using a platform called Behavox. As part of this project, we have to periodically review the unalerted communications, to ensure that they shouldn’t have been alerted, to ensure the system is working as intended. To do this, I have developed a Python script which gets data from their API, then gets the communication data (the actual content) from that data, then takes a sample, then outputs the sample as an excel. I am having an issue and don’t know if my script is working, I believe it is because of the way the pagination/cursor is working. Please rewrite my script below in full, correctly doing the pagination and cursor bits, and printing statements so that the user can see clearly how/why the pagination/cursor is working. For example I would like to see something like ‘Last element on page X had ID xyz, next element is yzx, so cursor is zxyz’) not exactly that but something similar to know what is going on. To fix this properly, you need to know what the comms object format is/what it looks like - the comms object is a list of dictionaries, and each dictionary looks like the following:
{“id”:”UIFT2763”,”datatype”:”whatsapp”,”alertIds”:[],”subject”:”None”,”date”:”2015-01-06T02:26:40Z”,”participants”:[],”attachments”:[]} with different data of course

CODE: import time
start_time = time.time()
import requests
 
import pandas as pd
import scipy.stats as st
 
from math import ceil
from datetime import datetime
 
import re
# - Can pull a sample of comms from the API using the below script
# - Note: that this sample does not include the actual content in a communication,
# due to a bug in the API not returning content data for a comm when the API is queried using
# start and end dates as search parameters.
# - Workaround: The content data is returned when using a content ID as the search parameter.
# therefore, generate the sample and then join in the content data by querying each content ID
# individually. Note, queries to the API should be ASCII encoded, but python strings are UTF. So
# used a regex-replace to replace + symbols with the ASCII equivalent (%2B)
 
API_TOKEN = 'z6XeAVcCz0vsv-GHX_r-SOWRZmPfIhKD0XEQFSEe-Ds'
SSL_CERT_PATH = "./glencore.behavox-saas.crt" # Export glencore.behavox-saas certificate and place in current directory otherwise SSL verification error may occur
 
API_LIMIT = 2000
 
def sample_size(N:int, C:float, E:float, p:float=0.5, round:bool=True):
    Z = st.norm.ppf(1 - (1 - C)/2)
    size = Z**2 * N * p * (1 - p) / (E**2 * (N - 1) + Z**2 * p * (1 - p))
    if round:
        size = ceil(size)
    return size
 
def get_comms_by_date(session, start_date:str, end_date:str, comms_per_page:int=API_LIMIT) -> pd.DataFrame:
    if comms_per_page > API_LIMIT:
        raise ValueError("Comms per page must be less than " +  str(API_LIMIT))
    base_url = f"https://glencore.behavox-saas.com/dashboard/api/3/communications/find?start-date={start_date}&end-date={end_date}&field=alertIds&limit={comms_per_page}"
    comms = []
    cursor = 1
    page = 0
    url = base_url
    while cursor:
        response = session.get(url, headers={"Authorization": "Bearer " + API_TOKEN}, verify=SSL_CERT_PATH)
        if response.status_code != requests.codes.ok:
            print("bad response")
            print(response.content)
            response.raise_for_status()
        if response.status_code == requests.codes.ok:
            print(f'good response at {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}')
        data = response.json()
        comms += data['communications']
        print("Comms length: " + str(len(comms)))
        if (cursor := data['pagination']['cursor']):
            url = base_url + '&cursor=' + cursor
            print(f"Page {page}: +{len(data['communications'])} communications.\n")
        else:
            print(f"Page {page}: +{len(data['communications'])} communications.\n\nNo more pages, {len(comms)} comms in total.\n")
 
        page += 1
    return pd.json_normalize(comms)
 
def get_comms_by_id(session, c_id:str) -> dict:
    url = f"https://glencore.behavox-saas.com/dashboard/api/3/communications?ids={c_id}"
    response = session.get(url, headers={"Authorization": "Bearer " + API_TOKEN}, verify=SSL_CERT_PATH)
    if response.status_code != requests.codes.ok:
        response.raise_for_status()
    return response.json()[0]
 
def main(save_file:bool=False):
    start_date = "2025-01-06T00:00:00.000Z"
    end_date = "2025-01-06T11:59:59.999Z"
 
    with requests.Session() as s:
        # get all comms in period
        df_all_comms = get_comms_by_date(s, start_date, end_date)
 
        # remove comms with no alert IDs
        df_unalerted_comms = df_all_comms[df_all_comms['alertIds'].apply(lambda x: len(x)) == 0]
 
        # sample unalerted comms
        size = sample_size(N=len(df_unalerted_comms), C=0.9, E=0.05)
        df_sample = df_unalerted_comms.sample(n=size)
        # replace "+" symbol in content IDs (utf-8) with the ASCII code for this symbol, as the utf-8 version throws errors when querying the API
        df_sample['id'] = df_sample['id'].str.replace('+', '%2B')
        # pull in text content for each communication
        comms_content = []
        for c_id in df_sample['id']:
            comms_content += [get_comms_by_id(s, c_id).get('content', {}).get('text','')]
        df_sample['content'] = comms_content
 
    if save_file:      
        df_sample.to_excel(f"output_sample.xlsx", index=False)
 
if __name__ == "__main__":
    main(save_file=True)

