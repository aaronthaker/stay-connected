from datetime import datetime
import math
import pandas as pd
from pathlib import Path

# Define sample size thresholds by population size:
SAMPLE_SIZE_THRESHOLDS = [
    (0, 500, 65),
    (501, 1000, 90),
    (1001, 2500, 110),
    (2501, 5000, 150),
    (5001, 10000, 215),
    (10001, float('inf'), 275)
]

START_DATE: str = "2025-05-25"
END_DATE: str = "2025-05-27"

FILE_PATH = Path(r"T:\99_Personal_Working_Folders\DS\QA\closed_alerts.csv")

def load_csv(path: Path, date_cols: list[str]) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")
    df = pd.read_csv(path, skiprows=[0], header=0)
    # Clean data - strip whitespaces and parse dates
    df.columns = df.columns.str.strip()
    if not date_cols:
        raise ValueError("Expect at least one date column but none were provided")
    for col in date_cols:
        df[col] = pd.to_datetime(df[col].str.strip(), format="%d/%m/%Y %H:%M:%S %z", utc=True, errors="raise")
    return df

def filter_alerts(df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame:
    start_dt = pd.to_datetime(start_date).tz_localize("UTC")
    end_dt = pd.to_datetime(end_date).tz_localize("UTC")
    return df[(df["Closing date"] >= start_dt) & (df["Closing date"] < end_dt)]

def determine_total_sample_size(total_alerts: int) -> int:
    """Determine total sample size based on total_alerts using predefined thresholds."""
    # Find the appropriate threshold range for total_alerts
    for lower, upper, sample_size in SAMPLE_SIZE_THRESHOLDS:
        if lower <= total_alerts <= upper:
            # Do not exceed the total number of alerts
            return min(sample_size, total_alerts)
    # Fallback (should not normally happen because of the float('inf') upper bound)
    return min(275, total_alerts)

def stratified_sampling_glencore(df: pd.DataFrame) -> pd.DataFrame:
    """Perform stratified sampling by Name, Current owner, and Month, 
    according to Glencore's proportional allocation method."""
    if df.empty:
        return df.iloc[:0]  # return empty DataFrame with same columns if no data
    
    # Add a 'Month' column for stratification (year-month of Closing date)
    df = df.copy()  # avoid modifying original
    df["Month"] = df["Closing date"].dt.strftime("%Y-%m")
    
    total_alerts = len(df)
    total_sample_size = determine_total_sample_size(total_alerts)
    
    # Group by scenario and analyst first
    sampled_rows = []  # to collect sampled DataFrame chunks
    for (scenario, owner), group_df in df.groupby(["Name", "Current owner"]):
        group_count = len(group_df)
        if group_count == 0:
            continue
        
        # Calculate proportional sample for this scenario-owner group
        raw_sample = (group_count / total_alerts) * total_sample_size
        # Determine how many samples to take (at least 1 if any alerts exist in group)
        if 0 < raw_sample < 1:
            desired_n = 1
        else:
            desired_n = int(round(raw_sample))
        if desired_n <= 0:
            desired_n = 1  # ensure at least one if group_count > 0
        if desired_n > group_count:
            desired_n = group_count  # cap to the group size available
        
        # Distribute the desired_n across months within this group (proportional to month counts)
        if "Month" in group_df.columns and group_df["Month"].nunique() > 1:
            month_counts = group_df["Month"].value_counts().to_dict()
            # Calculate raw allocation for each month
            month_alloc = {}
            floor_sum = 0
            remainders = []
            for month, count in month_counts.items():
                raw_month_share = (count / group_count) * desired_n
                floor_val = math.floor(raw_month_share)
                month_alloc[month] = floor_val
                floor_sum += floor_val
                remainders.append((raw_month_share - floor_val, month))
            # If total floor allocation is less than desired, distribute the remainder
            remainder_needed = desired_n - floor_sum
            if remainder_needed > 0:
                # Sort months by largest fractional remainder
                remainders.sort(reverse=True, key=lambda x: x[0])
                for i in range(min(remainder_needed, len(remainders))):
                    # Add one to the month with the largest remainder that still has alerts
                    _, month = remainders[i]
                    if month_alloc[month] < month_counts[month]:
                        month_alloc[month] += 1
            # If floor_sum > desired_n (unlikely), we could adjust down, but typically floor_sum <= desired_n.
            # Now sample from each month
            for month, n_samples in month_alloc.items():
                if n_samples <= 0:
                    continue
                # Cap n_samples to the number of alerts in that month group (safety check)
                n_samples = min(n_samples, month_counts.get(month, 0))
                month_group_df = group_df[group_df["Month"] == month]
                sampled_rows.append(month_group_df.sample(n=n_samples, random_state=None))
        else:
            # Only one month in group or month stratification not needed
            sampled_rows.append(group_df.sample(n=desired_n, random_state=None))
    
    # Concatenate all sampled sub-dataframes
    df_sample = pd.concat(sampled_rows) if sampled_rows else df.iloc[:0]
    return df_sample

def output_to_excel(df: pd.DataFrame, date_cols: list[str]) -> None:
    if not date_cols:
        raise ValueError("Expect at least one date column but none were provided")
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_filename = f"QA_Sample_{timestamp}.xlsx"
    df_to_save = df.copy()
    # Remove timezone information for Excel output
    for col in date_cols:
        if pd.api.types.is_datetime64_any_dtype(df_to_save[col]):
            df_to_save[col] = df_to_save[col].dt.tz_localize(None)
    df_to_save.to_excel(output_filename, index=False)
    print(f"\nSample results have been saved to '{output_filename}'.")

def print_breakdown(df_all: pd.DataFrame, df_sample: pd.DataFrame) -> None:
    """Print a pivot-style breakdown of QA taken vs total alerts per scenario and analyst."""
    # Group by scenario and analyst (ignore month for summary)
    total_counts = df_all.groupby(["Name", "Current owner"])["Alert ID"].count().reset_index(name="Total Alerts")
    sample_counts = df_sample.groupby(["Name", "Current owner"])["Alert ID"].count().reset_index(name="QA Alerts")
    summary_df = pd.merge(total_counts, sample_counts, on=["Name", "Current owner"], how="left")
    summary_df["QA Alerts"] = summary_df["QA Alerts"].fillna(0).astype(int)
    summary_df["QA Taken / Total"] = summary_df["QA Alerts"].astype(str) + " / " + summary_df["Total Alerts"].astype(str)
    # Create a pivot table style output for readability
    summary_pivot = summary_df.pivot(index="Name", columns="Current owner", values="QA Taken / Total").fillna("0 / 0")
    print(summary_pivot.to_string())  # print the pivot table
    
def main():
    # Define which date columns to parse and use
    date_columns = ["Creation date", "Closing date"]
    df = load_csv(FILE_PATH, date_columns)
    # Filter alerts by closing date range
    df_filtered = filter_alerts(df, start_date=START_DATE, end_date=END_DATE)
    # Perform stratified sampling using Glencore's method
    df_sample = stratified_sampling_glencore(df_filtered)
    # Output results to Excel
    output_to_excel(df_sample, date_columns)
    # Print breakdown of sample vs total alerts per scenario/analyst
    print_breakdown(df_filtered, df_sample)

if __name__ == "__main__":
    main()