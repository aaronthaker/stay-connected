import time
start_time = time.time()
import requests
import pandas as pd
import scipy.stats as st
from math import ceil
from datetime import datetime
import re

API_TOKEN = 'RDI3IW2-QF2YxGJ9LgEBeYY9oD7q59zvjuek6u8v_TM'
SSL_CERT_PATH = "./glencore.behavox-saas.crt"

# Function to calculate sample size
def sample_size(N: int, C: float, E: float, p: float = 0.5, round: bool = True):
    Z = st.norm.ppf(1 - (1 - C) / 2)
    size = Z ** 2 * N * p * (1 - p) / (E ** 2 * (N - 1) + Z ** 2 * p * (1 - p))
    if round:
        size = ceil(size)
    return size

# Function to check if an email is in the participants list
def contains_email(row, email_list):
    emails_in_participants = extract_emails(str(row['participants']))
    for email in emails_in_participants:
        if email.lower() in email_list:
            return True
    return False

# Function to extract emails from a string
def extract_emails(participants_str):
    email_regex = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
    return re.findall(email_regex, participants_str)

# Function to get communications by date
def get_comms_by_date(session, start_date: str, end_date: str, comms_per_page: int = 3000) -> pd.DataFrame:
    if comms_per_page > 3000:
        raise ValueError("Comms per page must be less than 3000")
    base_url = f"https://glencore.behavox-saas.com/dashboard/api/3/communications/find?start-date={start_date}&end-date={end_date}&field=alertIds&limit={comms_per_page}"
    comms = []
    cursor = 1
    page = 0
    url = base_url
    while cursor:
        response = session.get(url, headers={"Authorization": "Bearer " + API_TOKEN}, verify=SSL_CERT_PATH)
        if response.status_code != requests.codes.ok:
            response.raise_for_status()
        data = response.json()
        comms += data['communications']
        if (cursor := data['pagination']['cursor']):
            url = base_url + '&cursor=' + cursor
            print(f"Page {page}: +{len(data['communications'])} communications.")
        else:
            print(f"Page {page}: +{len(data['communications'])} communications.\n\nNo more pages, {len(comms)} comms in total.")
        page += 1
    return pd.json_normalize(comms)

# Function to get communications by ID
def get_comms_by_id(session, c_id: str) -> dict:
    url = f"https://glencore.behavox-saas.com/dashboard/api/3/communications?ids={c_id}"
    response = session.get(url, headers={"Authorization": "Bearer " + API_TOKEN}, verify=SSL_CERT_PATH)
    if response.status_code != requests.codes.ok:
        response.raise_for_status()
    return response.json()[0]

# Escape special characters in text
def escape_special_characters(text):
    if isinstance(text, str):
        return text.replace('\r', '\\r').replace('\t', '\\t').replace('\n', '\\n')
    return text

# Main function
def main(save_file: bool = False):
    start_date = "2024-09-04T00:00:00.000Z"
    end_date = "2024-09-04T23:59:59.999Z"
    filter_log = []  # List to store filtering actions

    with requests.Session() as s:
        df_all_comms = get_comms_by_date(s, start_date, end_date)
        total_comms = len(df_all_comms)
        filter_log.append(f"{total_comms} Comms pulled from API")

        # Load population list and filter by country (Singapore)
        df_population_list = pd.read_excel('ME_population.xlsx', header=0)
        df_population_list_filtered = df_population_list[df_population_list['COUNTRY'] == 'Singapore']
        email_list = []
        for emails in df_population_list_filtered['EMAIL']:
            email_list.extend([email.strip() for email in emails.split(',')])
        filter_log.append(f"{len(df_population_list) - len(df_population_list_filtered)} filtered out because they are not from Singapore")

        # Filter out communications from Singapore
        df_all_comms_singapore_filtered_out = df_all_comms[~df_all_comms.apply(contains_email, axis=1, args=(email_list,))]
        filter_log.append(f"{total_comms - len(df_all_comms_singapore_filtered_out)} filtered out due to matching Singapore emails")

        # Filter out communications with alerts
        df_unalerted_comms = df_all_comms_singapore_filtered_out[df_all_comms_singapore_filtered_out['alertIds'].apply(lambda x: len(x)) == 0]
        filter_log.append(f"{len(df_all_comms_singapore_filtered_out) - len(df_unalerted_comms)} filtered out due to having alerts")

        # Get sample size
        size = sample_size(N=len(df_unalerted_comms), C=0.9, E=0.05)
        df_sample = df_unalerted_comms.sample(n=size)
        df_sample['id'] = df_sample['id'].str.replace('+', '%2B')

        # Get communication content
        comms_content = []
        for c_id in df_sample['id']:
            comms_content += [get_comms_by_id(s, c_id).get('content', {}).get('text', '')]
        df_sample['content'] = comms_content

    # Write filtering log to text file
    if save_file:
        now = datetime.now().strftime("%Y-%m-%dT%H%M")
        with open("sampling_stats.txt", "a") as f:
            f.write(f"\n***** {now} *****\n")
            f.write(f"start: {start_date}\nend: {end_date}\n")
            for log in filter_log:
                f.write(log + "\n")
        
        # Escape special characters and save sample to Excel
        for col in df_sample.select_dtypes(include=[object]).columns:
            df_sample[col] = df_sample[col].apply(escape_special_characters)

        df_sample.to_excel(f"{now}_sample.xlsx", index=False)

if __name__ == "__main__":
    main(save_file=True)
    end_time = time.time()
    execution_time = end_time - start_time
    print(f"Execution time: {execution_time} seconds.")